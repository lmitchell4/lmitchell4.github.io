[
["index.html", "1 Introduction 1.1 Getting help 1.2 This is Version 1.0", " 1 Introduction We wrote this booklet as a companion to a one-day “Introduction to Statistics with R” course for fisheries biologists and ecologists. As we developed the materials for this course, we realized that we’re attempting to cover a range of topics that are usually taught over an entire semester. We try to present both statistics theory and how to use it in practice by writing code in R. This booklet is meant to help introduce topics and serve as reference material for students to continue learning about data analysis. Our approach to both the class and the booklet is to give a quick overview and a template to get you started. If you find that you need more information, you’ll have the tools and the vocabulary you need to find out more. 1.1 Getting help Everyone gets stuck sometimes. That’s true for both statistics and writing R code. Here are some things we do: 1. Use the help functions in R. They can be called using the ?. Example: “?mean” will bring up the help for the mean function. You can also use help(mean). 2. Use your search engine of choice. It’s never very satisfying when someone tells you that the way to find an answer is “just google it,” but that often helps. The tricky part, of course, is knowing what words to search for. That’s part of what this booklet is for- helping you figure out what to look up so that you can learn more. 3. Ask a friend for help. You might be thinking, “but I don’t have any friends who can help me work this out.” Not to worry! There is magic in explaining the problem to someone else, even if that person doesn’t use R or isn’t very interested in statistics. (Look up “rubber duck decoding” for proof.) 4. Walk away. Yes, really. Go do something else for a little while and you might be surprised that the solution comes to you when you’re thinking about something else. 1.2 This is Version 1.0 You’ve got the very first version of this booklet that we ever wrote and used in a course. Lucky you! We’re planning on using this material again, and probably revising as we do, so you might run across a newer version in the future. "],
["summarizing-data.html", "2 Summarizing data 2.1 Overview 2.2 Descriptive statistics 2.3 Data visualization 2.4 Publication quality graphics", " 2 Summarizing data 2.1 Overview Summarizing data is a way to get a general picture of the information contained in a dataset. The goal is to simplify the information, rather than focusing on the details. One analogy would be looking at a landscape from an airplane, rather than standing on the ground. From an airplane you can see the general features of the landscape- maybe pick out the floodplane of a river or see the edge of a city- that might be difficult to determine from the ground where you’re seeing details like trees, roads, and street signs. It’s important to start your analysis with this 30,000 foot view because it provides context for the details that we’ll be investigating later on. 2.2 Descriptive statistics There are a few essential characteristics of data that will become important when we talk about hypothesis testing and building linear models. Many of you will already be familiar with the concepts of summarizing data so we will use this section to also introduce the basics of the R coding language. R is a powerful tool for data manipulation, visualization, and analysis. R has many built-in calculations that you can use to help describe your data. Types of Data There are three basic types of data: Numeric data (“numeric”in R) Consists of a numbers and those numbers hold a numeric value (not categories that are identified by a number) Continuous data are data where any value is possible within the range, including decimals. For example, water depth is a continuous form of data. Interval data are numeric data, but data can only take specific values (usually integers) at regularly spaced intervals. For example, counts of fish are interval data because we usually can’t count partial fish. Categorical data (“factors” in R) Consists of data that are labeled by categories (e.g., yes/no or fish/mammal/bird). R calls the categories “factors” and it treats them differently than numeric data. More on that later, though. Caution: sometimes categorical data is labeled with numbers when we record them in a dataset (e.g., Treatments 1, 2, and 3). Be on the look out for these when you start analyzing your dataset! Ordinal data (“ordered factors” in R) Consists of ordered categories (e.g. high/medium/low) We won’t focus on this type of data in this course, but we’re mentioning it here so you know that it exists. The Center of your Data Mean When we say “mean” we’re usually referring to the arithmetic mean, or the “average” of a set of values. There are other types of means (e.g., the geometric mean), but we’re not concerned with those today. In statistics, it’s usually the arithmetic mean: \\({\\frac {1}{n}}\\sum _{i=1}^{n}a_{i}={\\frac {a_{1}+a_{2}+\\cdots +a_{n}}{n}}\\) The mean of a population is often represented by \\(\\mu\\) (the greek letter “mu”). A sample mean is often represented by \\(\\bar x\\). R has a built-in fuction for calculating the mean of a set of values. The function is “mean” and it takes one argument (a list of values) inside of the parentheses. mean(1:10) ## [1] 5.5 mean(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) ## [1] 5.5 mean(c(1, 2, 15, 3, 4, 15, 6, 7, 8, 19, 25, 15, 6)) ## [1] 9.692308 The arguments for the first two lines above are equivalent. In R, 1:10 is shorthand for a list of the numbers one through ten. Median Median is the middle observation of a dataset. There are an equal number of observations above and below the median. The function median is similar to the function mean in that it takes a single argument, which is also a list of values. median(1:10) ## [1] 5.5 median(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) ## [1] 5.5 median(c(1, 2, 15, 3, 4, 15, 6, 7, 8, 19, 25, 15, 6)) ## [1] 7 Notice that the mean and median are different for the third set of numbers. Mode The mode is the most common value in a dataset. This concept is generally most useful for categorical data, where you might want to see which category is most common. We will demonstrate the concept using the third set of values above. Unlike mean and median, R does not have a built-in function to identify the mode of a dataset. It does have a function called “mode,” but you’ll see that it identifies characteristics of the data itself. In this case, it tell us that our list is numeric. You can find the mode of a small set of values by using the table function. table returns a count of the number of times each value appears in your list of values. # the mode function in R does not calculate the mode of the data mode(c(1, 2, 15, 3, 4, 15, 6, 7, 8, 19, 25, 15, 6)) ## [1] &quot;numeric&quot; # the table function tells you the frequency of each value table(c(1, 2, 15, 3, 4, 15, 6, 7, 8, 19, 25, 15, 6)) ## ## 1 2 3 4 6 7 8 15 19 25 ## 1 1 1 1 2 1 1 3 1 1 For a larger set of values, you would not want to look through a large table. You can write a couple of lines of code to find the value with the largest count. # For ease, let&#39;s give our list a name x &lt;- c(1, 2, 15, 3, 4, 15, 6, 7, 8, 19, 25, 15, 6) # What is the most times any number appears in our data? max(table(x)) ## [1] 3 # Which value in the table appears 3 times? table(x)[which(table(x) == 3)] ## 15 ## 3 # Or you could use one line of code names(table(x))[table(x)==max(table(x))] ## [1] &quot;15&quot; An application where mode comes up in ecology is a bimodal distribution. A bimodal distribution means that the data has two values that are very common. This means that the mean and median values do not represent the most common or most expected values in your dataset. In the figure below, the most common value is 30, but the data has a second mode at 67. These values are both more common than the mean, which is 48.5. Figure 2.1: A bimodal data distribution (solid black line) with dotted vertical lines indicating the mean (grey) and two modes (black) Symmetry Data symmetry means that the two sides of a distribution look roughly the same. If a distribution is not symmetrical, it might be skewed to one side. For example, the figure below shows a symmetric distribution and a distribution with a long right tail (right skewed). Figure 2.2: A symmetric data distribution (black) and a right skewed distribution (grey) We will assess symmetry more in the data visualization section. One mathematical indication that data are skewed is if the mean and median are not very close in value. Using the same skewed distribution as above, the figure below shows that the mean of the distribution is larger than the median. This happens because the mean is more sensitive to extreme values than the median. If the mean is lower than the median, it indicates that the data are left-skewed. If you find that the mean and median are very different, it is important to investigate whether a single (or a few) extreme values are causing the skew. In the case where a few values are very high or very low, it is a good idea to investigate the data to see if there is a problem (e.g., a typo or equiment malfunction) before deciding how to proceed. Figure 2.3: A right skewed distribution (solid black line) with vertical dotted lines indicating the mean (black) and median (median). Variability or Spread The amount of variability or spread in a dataset is also very important. Two sets of data with the same mean might have very different amounts of variability. Summarizing the variability is the basis for calculating confidence intervals around means, comparing two or more datasets, building models, detecting changes over time, and many, many other things you might want to do with the data you collect. Understanding variability is a key to understanding more complex concepts. Variance Variance is often represented with \\(\\sigma^2\\) (greek letter sigma, squared). We won’t worry about calculating variance by hand, but here is the equation for sample variance for reference: \\(\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar{x})^2} {n-1}\\) where \\(\\bar{x}\\) is the mean of the data, using the formula given earlier. R can calculate variance for us using the function var. Here are two examples: var(1:100) ## [1] 841.6667 var(1:200) ## [1] 3350 Standard Deviation The standard deviation is another way we talk about varibility in a dataset. We often represent standard devaion as SD or \\(\\sigma\\) beacuse it is equal to the square root of the variance of a dataset: \\(SD = \\sqrt{\\sigma^2}\\) R can also calculate standard deviation for us, using the fucntion sd: sd(1:100) ## [1] 29.01149 We can show that the sd function gives the same answer as if we manually take the square root of the variance. This is also another example of nesting functions in R. Here, the argument to sqrt is the function var with the argument 1:100. sqrt(var(1:100)) ## [1] 29.01149 Coefficient of Variation The coefficient of variation (CV) is a measurement of the amount of variability in a dataset, relative to the mean. It’s usually expressed as a percentage. \\(CV = \\frac{SD}{\\bar{x}}\\cdot100\\%\\) CV is useful for comparing data with different means and variability or data with different units because it expresses the amount of variability in terms of the mean. For example, you might want to compare the variability in two environmental measurements. Maybe you want to compare the variability in water depth and water temperature in a pond. You can use the CV to compare variability, even though depth and temperature are measured in different units and they might have different means. R does not have a built-in function for CV, but we can write code to calculate it: # Create a list of values x &lt;- 1:100 # Write the equation for CV, using the functions sd and mean 100*sd(x)/mean(x) ## [1] 57.4485 Helpful R Function: summary The function summary is a generic function, which means that it can summarize many different kinds of arguments. If the argument that you give it is a list of numeric values, it calculates a 6-number summary of the data distribution. summary(1:100) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 25.75 50.50 50.50 75.25 100.00 If the argument is a list made up of words (characters), summary will tell you how many things are in the list and that it’s made up of characters. summary(c(&quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;)) ## Length Class Mode ## 10 character character If you wanted R to treat the words as categories (factors), you can explicitly tell R that by nesting the function as.factor inside of summary. The as.factor function will convert the list of words to a list of factors. (In R jargon, “convert” is often referred to as “coerce.”) When the argument to summary is a list of factors, summary returns the frequency of each factor level. summary(as.factor(c(&quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;))) ## bird fish mammal ## 2 6 2 In this case, the output from summary is similar to the output from table that we saw earlier. table(c(&quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;fish&quot;, &quot;mammal&quot;, &quot;bird&quot;)) ## ## bird fish mammal ## 2 6 2 2.3 Data visualization One of the best ways to get an overview of a dataset is to graph it. Graphing data helps you see patterns that might be important for your analysis and it gives you a broader context for any of the summary statistics that we just talked about. In this section, we will discuss some ways to graph (or plot) data and the information that each type of graph can help you find. Many tutorials and text books will beg you to “graph your data first”. There are many ways to make graphs in R. The biggest debate you’ll hear is “base graphics” v. “ggplot”. Each have their strengths, but we’re going to use base graphics (i.e., the built-in graphics package that comes with R) in this course. Why? We don’t have time to do both so we picked one. Keep in mind that making graphs is an iterative process. You’ll almost never make a graph in one try. Instead, plan on making a graph and then adjusting the code until the graph matches what you wanted to produce. # Make a toy dataset to work with set.seed(21) # make sure we always use the same values for reproducibility fish &lt;- data.frame(group = as.factor(rep(c(&quot;salmon&quot;, &quot;bass&quot;, &quot;smelt&quot;), each = 20)), fl = c(rnorm(n = 20, mean = 100, sd = 15), rpois(n = 20, lambda = 65), rnorm(n = 20, mean = 30, sd = 10))) fish$sal = 30*(fish$fl/max(fish$fl)) + rnorm(n = 60, mean = 10, sd = 5) # view the first few rows of data head(fish) ## group fl sal ## 1 salmon 111.89520 26.20020 ## 2 salmon 107.83377 44.30955 ## 3 salmon 126.19333 41.18596 ## 4 salmon 80.92996 30.86207 ## 5 salmon 132.96084 40.58935 ## 6 salmon 106.49696 34.41806 Above, we made a dataset named “fish” which has three variables: “group”, “fl”, which is short for fork length, and “sal”, which is short for salinity. We can use the summary function to learn about the dataset “fish”: summary(fish) ## group fl sal ## bass :20 Min. : 5.651 Min. : 5.355 ## salmon:20 1st Qu.: 36.584 1st Qu.:16.351 ## smelt :20 Median : 66.924 Median :25.425 ## Mean : 65.727 Mean :24.595 ## 3rd Qu.: 89.116 3rd Qu.:30.427 ## Max. :132.961 Max. :44.310 When we pass the dataset name to the summary function, it summarizes each variable in the dataset, just like if we passed it each variable separately. Since “group” is a factor, summary told us that there are 20 of each level in the dataset. Since “fl” and “sal” are numeric, it gave us a 6-number summary. 2.3.1 Visualizing the distribution of one variable Let’s make some visual summaries of the numeric variable “fl”. For now we’re not interested in the groups. Histograms A histogram groups the data into bins and the heights of the bars show the frequency of each bin (i.e., the count of values from your data that fall into each bin). Histograms are useful for looking at the shape of the distribution of the data. You can see things like the center, spread, and symmetry of the data. We’ll use the function hist to make a histogram. We’re going to use what is known as “dollar sign notation” to specify that the dataset fish contains the variable fl. It looks like this: “fish$fl”. A generic form is “dataset$variable”. hist(fish$fl) (#fig:hist1_fig)A histogram of fork length, using the default settings. The center of fl is around 70 and it’s fairly symmetrical. Maybe those boxes are a little too wide for our taste. We want to look at smaller bins, so let’s add an argument to hist that will make the boxes narrower. In R, when a function takes more than one argument, the arguments are separated by commas. In the example below, we’re naming the arguments “x” and “breaks” so that R will know which argument is which. If you aren’t sure which arguments a function can take, use the help in R. Use a question mark to search for a function. For example, “?hist” will bring up the help file for the hist function, with a list of the arguments that the function can take. hist(x = fish$fl, breaks = 16) (#fig:hist2_fig)A histogram of fork length with narrower bins. Boxplots Boxplots are another good way to see the center, spread, and symmetry of your data. They can also help you identify extreme values. The heavy line in the middle is the median. The top and bottom of the box are the third and first quartiles, respectively. This is called the “interquartile range” and it shows the middle half of the data. The whiskers are 1.5 times the distance between the first and third quartiles. If there are any extreme values outside the whiskers, they are marked with circles. The width of the box is not meaningful. boxplot(fish$fl) (#fig:boxplot1_fig)A boxplot of fork length. This data looks fairly symmetrical. The median line is pretty close to the center of the box. It’s a little higher than the center, but not very much. The whiskers are a little different in length, but we don’t have any extreme values. What if we want to look at a separate boxplot for each group? The boxplot function takes a formula for its first argument. The generic form of the formula is y ~ group, where y is your numeric variable of interest and group is the grouping variable. You still use the dollar sign notation, like we did before. Notice that we use the dataset name “fish” for both variables. boxplot(fish$fl ~ fish$group) (#fig:boxplot2_fig)A boxplot of fork lengths divided into groups. Q-Q Plots One question that comes up a lot is “are these data normal”? Often, we care about whether data is normally distributed because a statistical test that we are considering using assumes that the data are normally distributed. Q-Q Plots are a way to compare your data to a normal distribution. Q-Q stands for quantile-quantile, and you can think of it as “normal distribution quantiles v. sample quantiles”. Anywhere the points fall above the line, it means the value is more frequent than expected in a normal distribution. Below the line means it’s less frequent. # draw the graph of your data in Q-Q format qqnorm(fish$fl) # add a line that represents a perfectly normal dataset qqline(fish$fl, col = &quot;red&quot;, lwd = 2) Figure 2.4: A normal Q-Q plot of fork length. The middle of our data is pretty close to the line, but the two ends fall away from the line. There are more low values (fork lengths in the 20s) than we would expect and fewer high values than we would expect. 2.3.2 Visualizing two variables Plotting two variables together is a good way to look for relationships between variables. Some things to note when you plot two variables: 1. Do the points make a pattern (like a line or a curve) or are there points all over the plot? 2. If there is a pattern, is there a lot of scatter around it? 3. Are there more points on one area of the plot? 4. Are there any “funky” points that don’t fit the overall pattern? These might be mistakes or outliers and they need to be investigated. Keep in mind that human brains are very good at picking out patterns. It’s entirely possible to “see” something in the data that isn’t real or isn’t very influential. That’s why we will use quantitative methods for picking out patterns. (See the sections on inference and linear models.) Scatterplots The plot function makes graphs of two variables. The most basic type is a scatterplot. plot(x = fish$sal, y = fish$fl) (#fig:scatter1_fig)A scatter plot of salinity and fork length, using the default settings in the plot function. Line Plots R can also make a graph where the data are connected by lines, still using the function plot. Here, we specify type = “l” to tell R that we want a line graph. # sort the data in order of salinity fish &lt;- fish[order(fish$sal),] # draw a plot of the data connected by lines plot(x = fish$sal, y = fish$fl, type = &quot;l&quot;) (#fig:scatter2_fig)A line plot of salinity and fork length, using the default settings in the plot function. Bar Plots For categorical variables, you might want to graph the count of different groups in your data. We can create a bar graph of our fish group data using the functions barplot and table. This figure is pretty boring because each of the groups appears 20 times, but it gives you an idea of how the code works. barplot(table(fish$group)) (#fig:barplot_fig)A bar plot of fish groups. 2.4 Publication quality graphics 2.4.1 par The function par sets graphical parameters. There are many options and you can use ?par to view them all. Here, we have a few of the options that we use the most. Some of these parameters can also be set within a plot function. The option cex changes different things, depending on whether it is in a par() statement before a plot or within the plot() statement itself. Parameter Options What it does bty o, l, 7, c, u, j sets the shape of the box around the plot area. The shape of the letter describes which lines are drawn and which aren’t. cex a number scales the size of symbols and text lwd an integer, usually 1:5 sets the width of lines lty 1:6 changes the line type 1 is solid, others are kinds of dashes mar default: c(5, 4, 4, 2)+0.1 sets the width of the margins around a plot in the order bottom, left, top, right. mfrow any number of rows and columns makes a grid of plots in the form c(nr, nc) new TRUE plots over the previous plot lets you add new data to your plot pch integers 1:25 changes point symbols. See ?points for more information xaxs, yaxs i makes the x and y axis cross at 0 with no padding xaxt, yaxt “n” plots without drawing the x or y axis xlab, ylab text, in quotes sets the x and y axis labels xlog, ylog TRUE or FALSE TRUE makes the axis plot on a log scale (or use log = “x”) Let’s try out some of these parameters on our scatterplot example from earlier. par(cex = 1.5) #magnifies plot by 1.5 times plot(x = fish$sal, y = fish$fl, pch = 17, col = &quot;red&quot;, #makes the markers red triangles xlab = &quot;Salinity (psu)&quot;, ylab = &quot;Fork Lengh (mm)&quot;) (#fig:scatter3_fig)A scatter plot of salinity and fork length, with red triangles for markers. 2.4.2 Annotations The text function labels points. You write the plot function first and then the text function to put the text on top of the plot. The position options are 1, 2, 3, or 4 for bottom, left, top, right, respectively. plot(x = fish$sal, y = fish$fl, pch = 17, col = &quot;red&quot;, #makes the markers red triangles xlab = &quot;Salinity (psu)&quot;, ylab = &quot;Fork Lengh (mm)&quot;) text(x = fish$sal, y = fish$fl, #matches x and y in plot() labels = fish$group, pos = 1, #places text below the points. cex = 0.8) #makes the text smaller (#fig:scatter4_fig)A scatter plot of salinity and fork length, with red triangle markers and group labels. We can also change the colors based on the groups in our dataset by setting the color option to our group variable. plot(x = fish$sal, y = fish$fl, pch = 17, col = fish$group, #makes the markers red triangles xlab = &quot;Salinity (psu)&quot;, ylab = &quot;Fork Lengh (mm)&quot;) text(x = fish$sal, y = fish$fl, labels = fish$group, pos = 1, #places text below the points. cex = 0.8) #makes the text smaller (#fig:scatter5_fig)A scatter plot of salinity and fork length, with different colored symbols and text labels. Maybe we want a cleaner look with a legend for the colors instead of text labels. And we want to change the symbols because the default colors aren’t colorblind friendly. par(cex = 1.3) #magnify the symbols plot(x = fish$sal, y = fish$fl, pch = as.numeric(fish$group), #makes the colors vary by group. Need to coerce the factor levels to numbers. lwd = 2, #make the symbols thicker col = fish$group, #makes the markers vary by group. You could also coerce these to numbers, but works this way, too. xlab = &quot;Salinity (psu)&quot;, ylab = &quot;Fork Lengh (mm)&quot;, bty = &quot;l&quot;) legend(&quot;topleft&quot;, pch = 1:3, col = 1:3, pt.lwd = 2, legend = levels(fish$group)) (#fig:scatter6_fig)A scatter plot of salinity and fork length, that might be ready to go into a report. 2.4.3 Exporting graphics R can export figures as .bmp, .jpg, .tiff, .png, and .pdf files. There are functions to write each of these types of files and they are all pretty similar, with a few arguments that are specific to each one. We’ll give an example of writing a .png file. For each type of file, you write three chunks of code: the graphics device (png in this case), the code that makes the graph, and a function that closes the graphics device. It looks like this: # # 1. graphics window # png(filename = plot.png, # width = 6, height = 4, units = &quot;in&quot;, #4x6 inch figure # res = 90, #resolution = 90 ppi # bg = NA) #backround color = clear # # 2. plot code # par(cex = 1.1) #magnify a little bit # plot(x = fish$sal, y = fish$fl, # pch = as.numeric(fish$group), #makes the colors vary by group. Need to coerce the factor levels to numbers. # col = fish$group, #makes the markers vary by group. You could also coerce these to numbers, but works this way, too. # xlab = &quot;Salinity (psu)&quot;, # ylab = &quot;Fork Lengh (mm)&quot;) # legend(&quot;topleft&quot;, pch = 1:3, col = 1:3, legend = levels(fish$group)) # # # 3. close the graphics window to finish writing the file # dev.off() The code above is commented out so that loading this document doesn’t write an image file. You can run the code by copying it into your console without the leading # on each line. 2.4.4 Other resources We can’t fit everything here that you’ll need and there are already a lot of great summaries out there. Here are a few that we’ve found. Choose your favorite references and keep them handy. QuickR University of Illinois A microsoft version A blog post "],
["probability-distributions.html", "3 Probability distributions 3.1 Probability 3.2 Sampling 3.3 Common probability distributions", " 3 Probability distributions As a fisheries biologist, ecologist, or other applied user of statistics, it’s not essential that you have a deep understanding of the underlying theory of probability. It is helpful, however, to know something about probability distributions when you start to think about possible ways to analyze the data that you collect. Here, we focus on common probability distributions and where you might run into them in your daily work. If you’re interested in learning more about probability theory, most large statistics text books have at least one or two chapters devoted to it. 3.1 Probability The probability of an event is essentially how likely that event is to happen, or the relative frequency. The relative freqency can be expressed as the number of times an event occurs divided by the total number of events. \\(relative frequency = \\frac{frequency\\;of\\;event}{total\\;events}\\) That may seem obvious, but it’s worthwhile to give a mathematical definition because words like “probability” have multiple meanings in everyday speech. Notation There are many short-hand ways to write about probabilities. Here are a few examples of common ones: Notation Meaning \\(P(Y = 0) = 0.5\\) The probability that the variable Y equals 0 is 0.5. \\(P(0) = 0.5\\) The probability that whatever variable we’ve been talking about in the text equals 0 is 0.5. \\(P(2\\vert present)\\) The probability of observing a “2”, given that something is present \\(P(X = 2\\vert Z = 4)\\) The probability that X = 2, given that Z = 4 3.2 Sampling In biology and ecology, we apply the idea of probability and sampling to understand patterns. We measure a subset (take a sample) of the entire population of the thing we’re intersted in. In a statistical sense, a population could be a biological population of fish, but it could also be anything you’re interested in studying. A statistical population could be the entire set of possible locations on a wildlife refuge or all the islands in the Pacific Ocean. You, the researcher, define what the population of interest is and you select the sample from this population. When we take samples, we measure various things. Any quanitity that we summarize from these measurements is called a sample statistic. So for example, if you measure a bunch of fish and calculate the mean length, the mean is a statistic. The reason we calculate that mean length is not becuase we care about the mean length of the particular fish we caught - it’s because we want to learn about the bigger population of fish. A sample statistic is meant to help us estimate the mean of the whole population. Values that describe the entire population (such as the mean and variance) are called a population parameter. When we use a sample statistic to make deductions about a population paramter, we call that statistical inference. Sample statistics are random variables which, in practice, means that each time you take a sample, there is a chance that the value you observe will be different than before even though you’re sampling from the same population. How different each sample can be is determined by a set of rules. These rules include the possible outcomes and the probability of each outcome. The set of probabilities for the possible outcomes is the probability distribution. The example most text books use is flipping a regular coin: the possible outcomes are heads or tails, and the probability of each outcome is 50%. 3.3 Common probability distributions The table below highlights some common probability distributions that are useful for sampling in fisheries and ecology. This is a small subset of possible probability distributions. Distribution Data Type Possible Values Parameters Application Binomial Discrete \\(Success\\;(1),\\;Failure\\;(0)\\) \\(\\pi=P(1)\\) Any two categories Poisson Discrete \\(integers &gt; 0\\) \\(\\lambda= average\\;count\\) Counts Normal Continuous \\(all\\;real\\;numbers\\) \\(\\mu=mean\\) \\(\\sigma^2=variance\\) Continous variables Uniform Discrete or Continuous \\(it\\;depends\\) \\(\\mu=mean\\) Events that are equally probable What’s so special about being normal? The normal distribution is used a lot because 1. a lot of random variables come from (more or less) normal distributions 2. it has some convenient properties, like being symmetrical and having separate parameters for center and spread 3. if a sample is sufficiently large, the values become normally-distributed (central limit theorem) 4. if you add up a bunch of other distributions, the result is normally distributed When we talk about models, we’ll talk more about assumptions. Here’s an example of how it can be bad to assume that a distribution is normally distributed when it is actually something else. The graph below shows a density plot for a normal and poisson distribution with all parameter values set to 5. The most important thing to notice is that for the Poisson, the probability of observing a negative value is zero, but the Normal distribution can produce a negative number. If you assume the data are Normally distributed, you over-estimate the probability of getting a negative value quite a bit. (This is clearly a very simplified example. You’ll plot your data first and you won’t estimate your normal parameters this badly.) (#fig:dist_comp_fig)A normal distribution (N(5, 5); solid ine) and a poisson distribution (Pois(5), dashed line). Notation Probability distributions are often written as the name of the distribution or an abbreviation for it, followed by its parameters in parentheses. For example, a generic normal distribution can be written as \\(\\text{Normal}(\\mu, \\sigma^2)\\) or \\(\\text{N}(\\mu, \\sigma^2)\\) A standard normal distribution (a normal distribution where mean = 0 and variance = 1) would be written as \\(\\text{Normal}(0, 1)\\) or \\(\\text{N}(0, 1)\\) "],
["statistical-tests.html", "4 Statistical tests 4.1 Overview 4.2 Single sample tests 4.3 Two sample tests 4.4 Contingency tables", " 4 Statistical tests 4.1 Overview A statistical test provides a framework for formally testing a hypothesis about a population based on a sample data set collected from that population. There are four elements of a statistical test: the null hypothesis, \\(H_0\\) - the hypothesis to be tested. the alternative hypothesis, \\(H_a\\) - the hypothesis that will be accepted if the null hypothesis is rejected. the test statistic - a function of the data used to make a decision about the null hypothesis. the p-value - a probability used to draw a conclusion about the likelihood of the null hypothesis. 4.1.1 Large sample hypothesis testing with a normally distributed estimator Here’s an overview of the hypothesis testing framework for when you have a large sample from a normally distributed population: Suppose \\(\\hat{\\theta}\\) is an estimator for a quantity of interest, \\(\\theta\\), and that \\(\\hat{\\theta}\\) has a normal sampling distribution (approximately). A sample mean, \\(\\bar{y}\\), as an estimator for a population mean, is an example of an estimator that is normally distributed. Suppose we want to test if \\(\\theta\\) is equal to a given value \\(\\theta_0\\). Then a general hypothesis testing procedure would look like this: \\(H_0:\\) \\(\\theta = \\theta_0\\). Pick one of these three alternative hypotheses before looking at your data: \\[ H_a: \\left\\{ \\begin{array}{ll} \\theta &gt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\theta &lt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\theta \\ne \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Use the test statistic \\(Z = \\frac{\\hat{\\theta} - \\theta_0}{\\sigma_{\\hat{\\theta}}}\\). If the null hypothesis is true (and the underlying assumption that the estimator follows a normal distribution is true), then \\(Z\\) follows a standard normal distribution. For a given sample and null hypothesis, call this calculated value \\(z_{crit}\\). Calculate the p-value for this test, i.e., the probability of obtaining the test statistic we got, or a more extreme value, assuming the null hypothesis is true (we assumed \\(H_0\\) was true and used \\(\\theta_0\\) to calculate \\(z_{crit}\\)). The figure below shows a normal distribution with shaded tails reflecting all of these “more extreme” values for a two-tailed test. The shaded area gives the corresponding p-value. Figure 4.1: A standard normal distribution with shaded tails. The shaded area gives the p-value for a two-tailed hypothesis test. Note that a p-value isn’t the probability that the null hypothesis is true. In general, a “small” p-value means we would have a slim chance of getting the data we got if the null hypothesis was true. This suggests that your assumption (that the null hypothesis is true) is incorrect, in which case we would reject the null hypothesis. If the p-value is “large”, we typically say that we “fail to reject the null hypothesis” (as opposed to saying that we “accept the null hypothesis”). Whether a p-value is “small” or “large” is subjective. The classic rule of thumb is that you reject the null hypothesis when the p-value is less than 0.05 and fail to reject it otherwise (in which case we say you’re using a significance level of 0.05). You can always report your p-value and let your reader decide whether or not the null hypothesis should be rejected. 4.1.2 General hypothesis testing Hypothesis testing frameworks for other non-normal estimators are similar to the one presented here, but may have different assumptions about the sampling distribution of the estimator and/or the data itself. 4.2 Single sample tests 4.2.1 Tests for population mean and variance Most of the R functions described in this section and the following section have default values for arguments like mu. We show some of those arguments explicitly as a reminder that you can change them if needed. Z-test for a population mean: Hypotheses: \\[ \\text{H}_0: \\mu = \\mu_0. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\theta &gt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\theta &lt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\theta \\ne \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Sample size is large (maybe &gt; 30 or so observations?). Population variance is known. R Function: pnorm(q, mean=0, sd=1, lower.tail=TRUE) # single-tailed test 2*pnorm(q, mean=0, sd=1, lower.tail=TRUE) # two-tailed test Notes: In practice, population variance isn’t known and needs to be estimated, so this test is rarely used. Use the t-test instead. t-test for a population mean: Hypotheses: \\[ \\text{H}_0: \\mu = \\mu_0. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\theta &gt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\theta &lt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\theta \\ne \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Data are normally distributed. Population variance is being estimated (otherwise could use Z-test if sample size is large enough). R Function: t.test(x, alternative=alt, mu=mu, ...) Notes: Fairly robust to the normality assumption. Test for a population variance: Hypotheses: \\[ \\text{H}_0: \\sigma^2 = \\sigma_0^2. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\theta &gt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\theta &lt; \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\theta \\ne \\theta_0\\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Data are normally distributed. R Function: EnvStats::varTest(x, alternative=alt, sigma.squared=sigma_0, ...) Notes: Sensitive to the normality assumption. 4.2.2 Tests for normality 1) Normal QQ plot Compare ranked sample quantiles with those from a normal distribution. The points should fall on a straight line if the data are normally distributed. An S-shape or banana shape may indicate nonnormality. set.seed(4751) par(mfrow=c(1,2)) # Some example data that should theoretically pass the QQ plot normality &#39;test&#39;: y_norm &lt;- rnorm(n=200, mean=50, sd=3) qqnorm(y_norm, main=&quot;Normal QQ Plot: y_norm&quot;) qqline(y_norm) # default distribution is normal # Some example data that should theoretically fail the QQ plot normality &#39;test&#39;: y_lognorm &lt;- rlnorm(n=200, meanlog=0, sdlog=1) qqnorm(y_lognorm, main=&quot;Normal QQ Plot: y_lognorm&quot;) qqline(y_lognorm) Figure 4.2: Example QQ plots to investigate normality. 2) Shapiro-Wilk normality test Null hypothesis: the data come from a normally distributed population. Alternative hypothesis: the data do not come from a normally distributed population. shapiro.test(y_lognorm) ## ## Shapiro-Wilk normality test ## ## data: y_lognorm ## W = 0.4148, p-value &lt; 2.2e-16 The p-value is really small, so reject the null hypothesis that the data come from a normally distributed population. 4.3 Two sample tests Fisher’s F-test to compare two population variances: Hypotheses: \\[ \\text{H}_0: \\frac{\\sigma_x}{\\sigma_y} = r. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\frac{\\sigma_x}{\\sigma_y} &gt; r \\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\frac{\\sigma_x}{\\sigma_y} &lt; r \\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\frac{\\sigma_x}{\\sigma_y} \\ne r \\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Data are normally distributed. R Function: var.test(x, y, ratio=r, alternative=alt, ...) Notes: Sensitive to outliers. Student’s t-test to compare two population means (unpaired data): Hypotheses: \\[ \\text{H}_0: \\mu_x - \\mu_y = \\mu. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\mu_x - \\mu_y &gt; \\mu \\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\mu_x - \\mu_y &lt; \\mu \\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\mu_x - \\mu_y \\ne \\mu \\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Data are normally distributed. Population variances are the same. R Function: t.test(x, y, alternative=alt, mu=mu,paired=FALSE, ...) Wilcoxon rank-sum test to compare central locations of two samples (unpaired data): Hypotheses: \\[ \\text{H}_0: \\text{the two distributions differ by a location shift of} \\hspace{2pt} \\mu. \\] Assumptions: Samples are independent. (Note that this doesn’t assume the data are normally distributed.) R Function: wilcox.test(x, y, alternative=alt, mu=mu,paired=FALSE, ...) Notes: Non-parametric alternative to Student’s t test. Also known as Mann-Whitney test. Paired t-test to compare means from two paired samples (paired data): Hypotheses: \\[ \\text{H}_0: \\mu_x - \\mu_y = \\mu. \\\\ H_a: \\left\\{ \\begin{array}{ll} \\mu_x - \\mu_y &gt; \\mu \\text{,} &amp; \\hspace{3pt} \\text{upper-tail test} \\\\ \\mu_x - \\mu_y &lt; \\mu \\text{,} &amp; \\hspace{3pt} \\text{lower-tail test} \\\\ \\mu_x - \\mu_y \\ne \\mu \\text{,} &amp; \\hspace{3pt} \\text{two-tailed test} \\end{array} \\right. \\] Assumptions: Samples are independent. Samples are paired. Samples may be paired if they are taken on the same individual or taken from the same location. The differences are normally distributed. R Function: t.test(x, y, alternative=alt, mu=mu,paired=TRUE, ...) Why does it matter whether the data are paired or not? If paired data are positively correlated, then their covariance is positive, which means the variance used to calculate the test statistic is lower, which means it is eaiser to detect significant differences. 4.4 Contingency tables Suppose you have count data grouped into two categories and you want to investigate whether there is independence between the categories. Null hypothesis: Category 1 is independent of Category 2. Alternate hypothesis: Category 1 is not independent of Category 2. Here’s an example contingency table where category 1 reflects treatment levels ( treated or untreated) for a particular condition and category 2 reflects the outcome ( either a subject’s condition improved or not): Treated Untreated Row Totals Improved 315 108 423 Not improved 101 32 133 Column Totals 416 140 556 There are several ways to calculate test statistics (this list is not exhaustive): Pearson’s chi-square: this involves calculating expected counts (E) and comparing them to the observed counts (O) using a chi-square test statistic: \\(\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\). chisq.test(x, y, ...) chisq.test(x, y, p=p, ...) Fisher’s exact test: If any of the expected frequencies is less than 4 or 5, you should use Fisher’s exact test instead of Pearson’s chi-square test statistic. This is because \\(\\chi^2\\), as defined above, approximates a chi-square distribution when the sample size is large (enough). fisher.test(x, y, ...) "],
["statistical-modeling-overview.html", "5 Statistical modeling overview 5.1 Why fit a model? 5.2 Getting started 5.3 Overview of methods 5.4 Estimation methods", " 5 Statistical modeling overview 5.1 Why fit a model? Here are a couple of reasons you might want to fit a model: To explain something. For example, to determine what factors influence survival of an animal population. Or to determine if there is a difference in growth rates between two subpopulations. To predict something. For example, suppose we have determined a relationship between reproductive success of a fish population (e.g., number of larvae produced per female) and substrate type, and we want to predict reproductive success in the future under different habitat restoration scenarios. 5.2 Getting started Get to know your data set before starting any analyses. For example, maybe … Determine the data type of each field in your data set. Look for obvious errors (e.g., negative Secchi depths). Look for outliers, i.e., points that are clearly different from other observations but may or may not be “bad” or wrong. Some kind of plot can often help with identifying them. Outliers can be problematic in statistical. analyses even if they constitute valid data, so you may need to think carefully about how to handle them. Statistical reference books can give advice; you might also talk to colleagues/experts. Whenever reporting on analyses, remember to describe how you handled any outliers and why you took that approach. Consider how to deal with missing values. Look at correlations between numerical variables. Look at exploratory plots (the function pairs() can be useful). Try to have clear objectives for your analysis. It can be tricky to know where to start an analysis. It’ll help if you have a clear question or questions that you want to answer. If you’re working with data from an experiment, it’ll probably be easier to figure out your question than if you’re working with, say, data from a monitoring program. You can always revise your question(s) later. 5.3 Overview of methods Here’s a helpful overview of some common statistical modeling methods organized according to the nature of the explanatory and response variables (Crawley 2013). Determine which entry in the lefthand column best matches your data and the righthand column will give a suggestion for the type of analysis that may be appropriate/useful. Some common modeling methods organized by explanatory variable: Explanatory Variables Statistical Method All continuous Regression All categorical Analysis of variance (ANOVA) Combination of continuous and categorical Analysis of covariance (ANCOVA) Some common modeling methods organized by response variable: Response Variable Statistical Method Continuous Regression, ANOVA, ANCOVA Proportion Logistic regression Count Log-linear model Binary Binary logistic analysis 5.4 Estimation methods What does it mean to fit a model? It means finding parameters that give the “best fit” of the model to the data according to some quantifiable criteria. Least squares and maximum likelihood are two methods for finding parameters that give the “best fit”. 5.4.1 Least squares With the method of least squares, you find parameter estimates that minimize the sum of the squared differences between the observed values and their corresponding predicted values. We call this the sum of squared errors, SSE: \\[\\begin{align} SSE = ‎‎\\sum_{i=1}^{n} \\big( y_i - \\hat{y}_i \\big)^2. \\end{align}\\] Depending on the model, it may be possible to find the estimates analytically, otherwise numerical methods may be needed. One can technically use least squares without making assumptions about the distribution of the data, but if you want to make inferences about the resulting parameter estimates (e.g. test whether regression coefficients are equal to zero), then you need to make assumptions (usually the ones described in Chapter 6). 5.4.2 Maximum likelihood Overview of maximum likelihood: Given the data and a choice of model, what parameter values make the observed data most likely (i.e., maximize the probability of getting the data you got). Can find those parameters by writing out an expression for the likelihood of your data as a function of the parameters and maximizing that function. The general idea goes like this: Suppose you have \\(n\\) observations \\(y_i\\), \\(i = 1, ..., n\\) and the probability density for \\(Y\\) is given by \\(p(y)\\). Then, assuming the observations are independent and identically distributed, the probability of observing the whole data set is given by the product of the probabilities \\(p(y_i)\\). We call this quantity the likelihood and often represent it with something like \\(\\mathcal{L}\\): \\[\\begin{align} \\mathcal{L} = p(y_1) \\times p(y_2) \\times ... p(y_n) = \\prod_{i=1}^{n} p(y_i). \\tag{5.1} \\end{align}\\] The model parameters show up in \\(p(y)\\). For example, suppose we expect \\(Y\\) to follow a normal distribution. Then: \\[\\begin{align} P(Y=y) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp {\\Big(-\\frac{(y - \\mu)^2}{2 \\sigma^2}}\\Big), \\tag{5.2} \\end{align}\\] where \\(\\mu\\) and \\(\\sigma\\) will generally be the parameters we’re trying to estimate. If the expression \\(\\mathcal{L}\\) is really simple, we can write out the parameter estimates with a pencil and a sheet of paper using calculus, otherwise we will realistically need a computer. If the model is linear or linearizable (see Chapters 6 and 7), model-fitting software will generally use techniques from linear algebra to make the calculations relatively efficient. If the model isn’t linearizable, numerical optimization methods may be needed. (Note that we’re not covering Bayesian methods here.) 5.4.3 Important considerations Here’s a list of important considerations when fitting a model: Model assumptions Data transformations, standardizations, contrasts Model diagnostics These things apply to all kinds of models, but it’s easier to provide examples for very specific model types, so see Chapter 6. References "],
["ch-general-linear-models.html", "6 General linear models 6.1 Definitions and assumptions 6.2 Regression, ANOVA, and ANCOVA 6.3 Data standardization 6.4 Model selection 6.5 Diagnostics 6.6 Model fitting in R", " 6 General linear models 6.1 Definitions and assumptions Linear models are models that are linear in the parameters, not necessarily the explanatory variables (see point 3 below for a concrete example of what this means). The three assumptions of linear models are: All observations, \\(Y_i\\), are independent and identically distributed (iid). Errors are normally distributed with constant variance \\(\\sigma^2\\): \\[\\begin{align} Y_i = \\mu_i + \\epsilon_i, \\hspace{5pt} \\epsilon_i \\sim N(0, \\sigma^2). \\end{align}\\] There is a linear relationship between the dependent and independent variables. This means that the model is a linear function of the parameters to be estimated, not of the independent variables. For example, the following expression is quadratic in \\(x\\), but is linear in the parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\): \\[\\begin{align} \\mu_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2. \\end{align}\\] 6.2 Regression, ANOVA, and ANCOVA Regression, analysis of variance (ANOVA), and analysis of covariance (ANCOVA) all fall under the category of general linear models. They differ in the nature of the explanatory variables used, but follow the same basic model framework. In regression, all of the explanatory variables are continuous. In ANOVA, they are all categorical. A categorical variable takes on a limited number of possible values and is often qualitative rather than quantitative. A simple example is sex (with two possible values, male and female). In ANCOVA, there are both continuous and categorical variables. The goal of regression is to determine which explanatory variables impact the overall mean, while the goal of ANOVA is to compare the means of different groups (e.g., different experimental treatment groups) against each other. In ANCOVA, we are again interested in determining which variables impact the overall mean, but we also take into account differences between groups. A quick note on categorical variables: In regression, categorical variables (a.k.a. factors) are handled by mapping them to a set of numeric variables known as contrasts. For example, if you have two levels of a categorical variable, \\(x\\), then within the model you can let \\(x\\) map to 0 when it takes on the first level, and to 1 when it takes on the second level; this would be a treatment contrast, which is the default in R, though there are other options. 6.3 Data standardization You will generally want to standardize numeric explanatory variables in order to help reduce uncertainty in your parameter estimates and potentially avoid computational problems if your original data values are really large or really small. This typically means subtracting the mean and dividing by the standard deviation: \\[\\begin{align} x_{standardized} = \\frac{x - \\text{mean}(x)}{\\text{sd}(x)}. \\end{align}\\] 6.4 Model selection 6.4.1 Hypothesis testing We can investigate whether or not a variable should stay in the model by testing if its coefficient is equal to 0. As we’ll see a little later on, R can do this work for us, but if there are multiple variables in the model we have some options about how to proceed with a hypothesis test. For example, suppose the full model we’re considering has 3 explanatory variables, \\(x\\), \\(w\\), and \\(z\\). Then should we test if the coefficient of \\(x\\) is zero when only \\(x\\) is included in the model, or when both \\(x\\) and \\(w\\) are in the model, or \\(x\\) and \\(z\\), or all three variables? There isn’t a clear cut answer, but when you are carrying out a hypothesis test on one variable you should know what assumptions are being made about the inclusion or exclusion of the other variables. In statistics references, you’ll commonly see these categories of hypothesis testing procedures (among others): Type I: Sequential sum of squares: Variables are added in to the model one at a time in a specific order, starting from an intercept-only model. The significance of a particular variables is then tested assuming the preceding variables are already in the model. Type III: Partial sum of squares: The significance of a variable is tested assuming all the other variables are already in the model, excluding any terms (e.g., interactions) that contain the variable being tested. So far we’ve been discussing testing the significance of a single coefficient. It’s possible to test more than one coefficient at a time, though we won’t cover that here. 6.4.2 AIC Akaike’s information criteria (AIC) can be used to compare and rank nested models. Generally, two models are nested if one contains all the same terms as the other plus some additional terms. As a simple example, these two models are nested: \\[\\begin{align} Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x + \\beta_2 z, \\sigma^2) \\end{align}\\] \\[\\begin{align} Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2) \\end{align}\\] AIC is calculated using the model log-likelihood, \\(\\mathcal{L}\\), and the number of parameters in the model, \\(p\\): \\[\\begin{align} AIC = -2 \\times \\mathcal{L} + 2 \\times (p + 1). \\end{align}\\] In general, a smaller AIC indicates a better fit. You can improve the fit by including more data (sometimes even with irrelevant data), so the second term, \\(2 \\times (p + 1)\\) serves as a penalty: if the increase in likelihood from adding more parameters doesn’t outweigh the penalty, AIC will increase. A general rule of thumb is that a change in AIC of at least 2 is signficant. Note that AIC is defined up to an additive constant that cancels out when you take the difference between two AIC values. 6.5 Diagnostics We use diagnostic tools to assess the goodness-of-fit of a model (that is, to investigate how well the model describes the data) and to look for model assumption violations, influential points, etc. 6.5.1 Residuals Residuals are one of the most useful diagnostic tools. For observation \\(y_i\\) with model predicted (or fitted) value \\(\\hat{y}_i\\), the corresponding residual is given by: \\[\\begin{align} r_i = y_i - \\hat{y}_i. \\end{align}\\] Patterns in residual plots can indicate poor fit and/or violation of the assumptions of normality and constant variance. Here’s a visual example. Both of the figures below show hypothetical residuals plotted against fitted values. The figure on the left shows pattern-free residuals centered around 0, which is what you want. The figure on the right shows residuals increasing as the fitted values increase, which is a sign that your constant variance assumption has been violated. Transforming your observations, \\(y_i\\), can sometimes help with this. For example, you could try fitting the model using \\(\\log(y_i)\\) or \\(\\sqrt{y_i}\\) in place of \\(y_i\\). Figure 6.1: Example residual vs. fitted value plots showing no pattern (left) and an increasing pattern (right). These figures show hypothetical residuals plotted against the single explanatory variable \\(x\\). The left plot again shows no pattern, which is what you want, while the right plot shows curvature indicating that additional explanatory terms (for example, perhaps a quadratic term, \\(x^2\\)) may be needed. Figure 6.2: Example residual vs. explanatory variable plots showing no pattern (left) and a curved pattern (right). To check the assumption that the errors are normally distributed, you can use a qq plot: Figure 6.3: Example residual qq plots showing agreement with normality (left) and a departure from normality (right). We designed these examples so it would be relatively easy to spot the “good” (pattern free) versus “bad” (not pattern free) residual plots. In practice, it’ll be harder. 6.5.2 Coefficient of determination, \\(R^2\\) The coefficient of determination, \\(R^2\\), is the proportion of the total variation in the response variable, \\(Y\\), that is accounted for by a regression model. The way it is defined, \\(R^2\\) will fall between 0 and 1, and generally, the closer to 1 it is the “better” (conditional on other model diagnostics looking ok). The coefficient of determination can often be increased by putting more variables into the model, so people will sometimes use an adjusted \\(R^2\\), \\(R^2_{adj}\\), that takes into account the number of observations and number of model parameters, and can decrease as the number of model parameters increases. 6.6 Model fitting in R 6.6.1 Linear regression Here’s an example of fitting linear regression models in R. The data are contained in a data frame called dat. There are 40 observations. Below we show the first six rows of the data set and a plot of the dependent variable versus the independent variable: head(dat) ## x y ## 1 3.982630 8.212405 ## 2 5.581858 6.364092 ## 3 8.592800 4.731792 ## 4 13.623117 10.394223 ## 5 3.025229 7.532588 ## 6 13.475845 23.079144 plot(dat$x, dat$y, xlab=&quot;x&quot;, ylab=&quot;y&quot;) Figure 6.4: Data from the linear regression example. Suppose we start by fitting a simple linear regression (the term “simple linear regression” just means that there’s at most one explanatory variable). The model looks like this: \\[\\begin{align} Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x, \\sigma^2). \\end{align}\\] We can use the function lm() to fit the model. The first argument is a formula describing the model, starting with the name of the response variable, in this case y, followed by a tilde, ~, then the independent variable terms separated by plus signs, +. Here we only have one independent variable, x. The second argument is the name of the data frame in which to find the variables, in this case dat. ## Fit the model using the function lm(): fit0 &lt;- lm(y ~ x, dat) Printing out the model object directly will show the parameter estimates: fit0 ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Coefficients: ## (Intercept) x ## -1.016 1.329 Pass it to the summary() function to get more information like coefficient standard errors, p-values from partial sum of squares tests on the coefficients, \\(R^2\\) values, and the p-value from an F test on the overall fit of the model relative to a model with only an intercept (the null hypothesis is that this model and an intercept-only model describe the data equally well). summary(fit0) ## ## Call: ## lm(formula = y ~ x, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.7331 -6.2604 -0.1834 5.4225 14.5125 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.0163 2.5510 -0.398 0.693 ## x 1.3292 0.2911 4.567 5.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.644 on 38 degrees of freedom ## Multiple R-squared: 0.3543, Adjusted R-squared: 0.3373 ## F-statistic: 20.85 on 1 and 38 DF, p-value: 5.088e-05 Running the funtion anova() on an lm object will carry out sequential sum of squares: anova(fit0) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x 1 1218.6 1218.57 20.854 5.088e-05 *** ## Residuals 38 2220.4 58.43 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Calling the plot() function on an lm object will produce four diagnostic plots: par(mfrow=c(2,2)) plot(fit0) Figure 6.5: Diagnostic plots produced by plot(). The top two panels in the figure above are a residual and Q-Q plot like we saw before. The bottom left panel shows (square root) standardized residuals against fitted values; if there is a pattern in the residuals, this may indicate the constant variance assumption has been violated (this kind of plot is similar to the one in the first panel). The bottom right plot shows standardized residuals against leverage, which is a measure of how influential a point is. The higher the leverage, the more of an effect a given point has on the overall fit. The plot() function will label points with particularly high leverage. There’s a clear U-shaped pattern in our residuals. Looking at our plot of \\(y\\) versus \\(x\\) again, it appears that \\(y\\) is a nonlinear function of \\(x\\), so we may want to try fitting a polynomial model. Let’s try a cubic polynomial model: \\[\\begin{align} Y \\sim \\text{Normal}(\\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3, \\sigma^2). \\end{align}\\] We can start by standardizing the numeric variable \\(x\\) (this is important for polynomial variables): # You can standardize data manually: dat$xx &lt;- (dat$x - mean(dat$x))/sd(dat$x) # Or maybe use a built-in function: # dat$xx &lt;- scale(dat$x)[ ,1] You could define \\(x^2\\) and \\(x^3\\) fields in dat and use those, or you can use the function I(), which allows you to define new model terms on the fly. ## Fit the model using the function lm(): fit &lt;- lm(y ~ xx + I(xx^2) + I(xx^3), dat) Look at a summary of the new model: summary(fit) ## ## Call: ## lm(formula = y ~ xx + I(xx^2) + I(xx^3), data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.254 -3.301 1.051 3.812 13.270 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.86687 1.40352 2.755 0.00915 ** ## xx 5.75648 2.21053 2.604 0.01330 * ## I(xx^2) 5.51284 1.06910 5.157 9.3e-06 *** ## I(xx^3) -0.06259 1.12583 -0.056 0.95598 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.949 on 36 degrees of freedom ## Multiple R-squared: 0.6295, Adjusted R-squared: 0.5987 ## F-statistic: 20.39 on 3 and 36 DF, p-value: 6.796e-08 Maybe look at the sequential sum of squares for the new model: anova(fit) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## xx 1 1218.57 1218.57 34.4341 1.043e-06 *** ## I(xx^2) 1 946.35 946.35 26.7417 8.895e-06 *** ## I(xx^3) 1 0.11 0.11 0.0031 0.956 ## Residuals 36 1273.99 35.39 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The function AIC() will calculate AIC. Our results so far indicate that the cubic term is not contributing to the model fit so let’s fit a model without it and recalculate AIC: fit2 &lt;- lm(y ~ xx + I(xx^2), dat) AIC(fit2) ## [1] 259.9596 summary(fit2) ## ## Call: ## lm(formula = y ~ xx + I(xx^2), data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.276 -3.319 1.017 3.845 13.322 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.8637 1.3833 2.793 0.00822 ** ## xx 5.6456 0.9397 6.008 6.13e-07 *** ## I(xx^2) 5.5167 1.0523 5.242 6.66e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.868 on 37 degrees of freedom ## Multiple R-squared: 0.6295, Adjusted R-squared: 0.6095 ## F-statistic: 31.43 on 2 and 37 DF, p-value: 1.052e-08 AIC(fit)- AIC(fit2) ## [1] 1.996566 There was a change in AIC of approximately 2 and now all of our model terms are significant at the 0.01 level (based on partial sum of squares). The function stepAIC() attempts to find the “best” model by stepping through different subsets of the model you pass it and comparing their AIC values. (Using stepAIC is overkill for this simple example, but that’s ok.) library(MASS) MASS::stepAIC(fit) ## Start: AIC=146.44 ## y ~ xx + I(xx^2) + I(xx^3) ## ## Df Sum of Sq RSS AIC ## - I(xx^3) 1 0.11 1274.1 144.44 ## &lt;none&gt; 1274.0 146.44 ## - xx 1 239.98 1514.0 151.34 ## - I(xx^2) 1 940.98 2215.0 166.56 ## ## Step: AIC=144.44 ## y ~ xx + I(xx^2) ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 1274.1 144.44 ## - I(xx^2) 1 946.35 2220.4 164.66 ## - xx 1 1242.88 2517.0 169.68 ## ## Call: ## lm(formula = y ~ xx + I(xx^2), data = dat) ## ## Coefficients: ## (Intercept) xx I(xx^2) ## 3.864 5.646 5.517 This is also indicating that the simpler quadratic model in \\(x\\) is adequate. It’s helpful to know how to extract information from model objects: ## Extracting model components: names(fit) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; fit$coefficients ## (Intercept) xx I(xx^2) I(xx^3) ## 3.86686591 5.75648381 5.51283699 -0.06258567 fit$residuals ## 1 2 3 4 5 6 ## 5.0654230 3.9906433 -0.5708063 -12.2535686 3.1377245 1.1557533 ## 7 8 9 10 11 12 ## 1.0631925 -10.9362873 -4.1717943 2.3165524 8.2963689 -1.8365220 ## 13 14 15 16 17 18 ## 2.2431129 -1.8607570 -9.4589799 -4.2441089 -3.0103251 2.4201994 ## 19 20 21 22 23 24 ## 6.2156793 5.5912555 0.8665530 -3.0065450 4.1823478 2.9809425 ## 25 26 27 28 29 30 ## -6.1922101 -6.4362827 2.2140174 3.8954107 0.4175585 4.6962755 ## 31 32 33 34 35 36 ## 1.4118222 -5.2588421 1.0386600 -9.0560737 10.7685309 13.2698814 ## 37 38 39 40 ## -2.1646544 -8.1458956 3.7844070 -2.4186589 The function lm() produces fitted values calculated with the data used to fit the model. If you want model predicted values for different inputs, you can use the predict() function. fit$fitted.values ## 1 2 3 4 5 6 7 ## 3.146982 2.373449 5.302598 22.647792 4.394864 21.923391 25.451043 ## 8 9 10 11 12 13 14 ## 8.362105 7.136444 9.211083 4.292381 5.048558 9.481249 2.388288 ## 15 16 17 18 19 20 21 ## 13.634199 3.541260 10.906267 29.344335 2.380953 14.062368 24.667051 ## 22 23 24 25 26 27 28 ## 4.149822 7.994944 6.659285 3.121638 2.392783 11.549160 2.384907 ## 29 30 31 32 33 34 35 ## 19.879903 2.433371 3.274612 6.118521 3.466924 4.786313 17.068511 ## 36 37 38 39 40 ## 8.679584 15.035886 7.304134 11.205334 2.497690 # The new data should be in a data frame with the same field names used in the model: new_data &lt;- data.frame(&quot;xx&quot;=seq(from=min(dat$xx), to=max(dat$xx), by=0.1)) new_data$predicted_y &lt;- predict(fit, newdata=new_data) head(new_data) ## xx predicted_y ## 1 -1.787513 11.549160 ## 2 -1.687513 10.152385 ## 3 -1.587513 8.872203 ## 4 -1.487513 7.708240 ## 5 -1.387513 6.660119 ## 6 -1.287513 5.727465 plot(dat$xx, dat$y) points(dat$xx, fit$fitted.values, col=&quot;blue&quot;) points(new_data$xx, new_data$predicted_y, col=&quot;orange&quot;) legend(&quot;topleft&quot;, c(&quot;Observation&quot;,&quot;Predicted values from lm()&quot;, &quot;Predicted values from predict()&quot;), col=c(&quot;black&quot;,&quot;blue&quot;,&quot;orange&quot;), pch=1) Figure 6.6: Observations and model predicted values. vcov(fit) ## (Intercept) xx I(xx^2) I(xx^3) ## (Intercept) 1.96985585 0.102523 -1.11361583 -0.06420251 ## xx 0.10252304 4.886465 -0.12787903 -2.24573658 ## I(xx^2) -1.11361583 -0.127879 1.14296630 0.07867607 ## I(xx^3) -0.06420251 -2.245737 0.07867607 1.26750372 names(summary(fit)) ## [1] &quot;call&quot; &quot;terms&quot; &quot;residuals&quot; &quot;coefficients&quot; ## [5] &quot;aliased&quot; &quot;sigma&quot; &quot;df&quot; &quot;r.squared&quot; ## [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot; &quot;cov.unscaled&quot; summary(fit)$r.squared ## [1] 0.6295491 summary(fit)$adj.r.squared ## [1] 0.5986782 Calling the plot() function on an lm object will produce four diagnostic plots: par(mfrow=c(2,2)) plot(fit) Figure 6.7: Diagnostic plots produced by plot(). These residual plots look better than before since they don’t reflect a clear pattern. 6.6.2 ANOVA For an ANOVA example we’ll use the yields data set from Crawley (2013). These data reflect an experiment in which crop yield per unit area is investigated for different soil types (sand, clay, or loam) with other factors (e.g., seed variety, fertilizer, etc.) consistent between soil plots. There are 10 observations for each soil type. Here are the data: yields &lt;- read.table(file.path(&quot;data&quot;,&quot;yields.txt&quot;), header=TRUE) yields ## sand clay loam ## 1 6 17 13 ## 2 10 15 16 ## 3 8 3 9 ## 4 6 11 12 ## 5 14 14 15 ## 6 17 12 16 ## 7 9 12 17 ## 8 11 8 13 ## 9 7 10 18 ## 10 11 13 14 We’re interested in whether mean yield differs by soil type. To get a feel for the data we can calculate the group means and variances: sapply(yields, mean) ## sand clay loam ## 9.9 11.5 14.3 sapply(yields, var) ## sand clay loam ## 12.544444 15.388889 7.122222 and use boxplot() to plot the data: boxplot(yields, col=&quot;green3&quot;, ylab=&quot;Yield&quot;) Figure 6.8: Boxplot of the crop yield data. The boxes for sand and loam don’t overlap, so that’s a hint that there may be a difference between those groups, but it’s hard to tell for clay. The data set is currently in what we call “wide” format because the groups are represented in separate columns. For passing the data to model fitting functions, it will be helpful to have it into “long” format, which we can accomplish with the stack() function: yields_long &lt;- stack(yields) head(yields_long) ## values ind ## 1 6 sand ## 2 10 sand ## 3 8 sand ## 4 6 sand ## 5 14 sand ## 6 17 sand # Give yields_long more useful names: names(yields_long) &lt;- c(&quot;Yield&quot;,&quot;SoilType&quot;) head(yields_long) ## Yield SoilType ## 1 6 sand ## 2 10 sand ## 3 8 sand ## 4 6 sand ## 5 14 sand ## 6 17 sand The underlying assumptions for ANOVA are that the populations are normally distributed and variance is constant (which implies that each group has the same variance). The data look satisfactorily normal: qqnorm(yields_long$Yield) qqline(yields_long$Yield, lty=2) Figure 6.9: Checking normality of the crop yield data. We can test the equal variance assumption using fligner.test(), which tests the null hypothesis that the variances in the groups are the same: fligner.test(Yield ~ SoilType, data=yields_long) ## ## Fligner-Killeen test of homogeneity of variances ## ## data: Yield by SoilType ## Fligner-Killeen:med chi-squared = 0.36507, df = 2, p-value = ## 0.8332 The p-value, 0.8332, is relatively large (above any typical significance level, e.g., 0.01, 0.05, 0.1, etc.), so we fail to reject the null hypothesis that the group variances are the same. Now we can use aov() or lm() to carry out the ANOVA: yield_aov &lt;- aov(Yield ~ SoilType, data=yields_long) summary(yield_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## SoilType 2 99.2 49.60 4.245 0.025 * ## Residuals 27 315.5 11.69 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The degrees of freedom for the treatment (i.e., SoilType) sum of squares is equal to the number of treatments groups minus 1, 3 - 1 = 2. The degrees of freedom for the error sum of squares is the total number of observations minus the number of treatment groups, 30 - 3 = 27. And finally, the total degrees of freedom (not shown as part of the output of summary()) is the total number of observations minus 1, 30 - 1 = 29. The null hypothesis for the F test run by summary() is that the treatment means are not significantly different and the alternative hypothesis is that at least one of the means is different (without specifying which one(s) are different). There’s a relatively small probability of getting the results we got (0.025) assuming the null hypothesis is true, so we reject the null hypothesis and conclude that not all of the treatment means are the same. You can also check the ANOVA assumptions after fitting the model using plot(): par(mfrow=c(2,2)) plot(yield_aov) Figure 6.10: Diagnostic plots for the ANVOA model. Patterns in the top left panel would indicate a violation of the constant variance assumption and non-normality would be reflected in the top right panel. The standardized residuals in the bottom two panels look ok and there don’t appear to be influential points. We said above that we could use lm() instead of aov(). This is because linear regression and ANOVA are fundamentally the same thing; they just differ in the nature of the explanatory variables. yield_lm &lt;- lm(Yield ~ SoilType, data=yields_long) summary(yield_lm) ## ## Call: ## lm(formula = Yield ~ SoilType, data = yields_long) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.5 -1.8 0.3 1.7 7.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.900 1.081 9.158 9.04e-10 *** ## SoilTypeclay 1.600 1.529 1.047 0.30456 ## SoilTypeloam 4.400 1.529 2.878 0.00773 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.418 on 27 degrees of freedom ## Multiple R-squared: 0.2392, Adjusted R-squared: 0.1829 ## F-statistic: 4.245 on 2 and 27 DF, p-value: 0.02495 The output of this call to summary() is a little different than before. The “Intercept” estimate represents the mean yield for the sand group, while the SoilTypeclay estimate represents the difference between the mean yield of sand and clay. Similarly, the SoilTypeloam estimate represents the difference between the mean yield of sand and loam. To verify this, we could recalculate our group means: sapply(yields, mean) ## sand clay loam ## 9.9 11.5 14.3 sapply(yields, mean) - mean(yields$sand) ## sand clay loam ## 0.0 1.6 4.4 When differences between group means are tested in this way its known as a treatment contrast. The F test at the bottom of the output of summary(yield_lm) tests the null hypothesis that an intercept-only model fits the data as well as this model. Note that an intercept-only model implies that the treatment groups all have the same mean, which is what we wanted to test (and did test once already using aov()). As expected, the p-value is the same as we found before (0.025) so we reject the hypothesis that the treatment means are all the same. It’ll matter whether you use aov() or lm() if you have pseudoreplication, for example if repeated measurements are taken on the same individual. We won’t go into the details of pseudoreplication here, but know that the Error() function (?Error) exists for handling this case. References "],
["ch-nonlinear-models.html", "7 Nonlinear models 7.1 Why distinguish between linear and nonlinear models? 7.2 Nonnormal errors", " 7 Nonlinear models Not all models are linear. The Ricker model, for example, is commonly used in the field of fisheries to describe density-dependent population growth: \\[\\begin{align} f(x) = a \\hspace{1pt} x \\hspace{1pt} e^{-bx}. \\end{align}\\] Note that this function is nonlinear in the independent variable \\(x\\). 7.1 Why distinguish between linear and nonlinear models? The reason we separate out general linear models from nonlinear models is because general linear model parameter estimates can be calculated relatively efficiently with techniques from linear algebra. One convenient side effect is that you don’t need to plug in initial values for the parameters. For nonlinear models, we need numerical optimization techniques that require initial values and may return different parameter estimates depending on the initial values. Numerical optimizers may also fail to find parameter estimates. In other words, linear models are generally easier to fit. 7.2 Nonnormal errors Recall that one of the assumptions of linear models is that the errors are normally distributed. If your data are not normally distributed but fall into a limited set of distributions that are said to belong to the exponential family (including the Poisson, binomial, and Gamma distributions), then you may be able to transform your model into a model that can be fit more easily than a typical nonlinear model. These kinds of models are called generalized linear models, or GLM’s for short. We won’t have time to cover the details of GLM’s here, but a starting place to look in R is the glm() function: ?glm The negative binomial distribution technically isn’t part of the exponential family of distributions, but negative binomial models can be fit by extending the idea behind GLM’s. See the glm.nb() function in the MASS library: library(MASS) ?glm.nb "],
["resources.html", "8 Resources", " 8 Resources We used these resources to put this document together: A First Course in Statistical Programming with R by W. John Braun and Duncan J. Murdoch, Cambridge University Press, New York, 2009. Ecological Models and Data in R by Benjamin M. Bolker, Princeton University Press, New Jersey, 2008. https://ms.mcmaster.ca/~bolker/emdbook/ The R Book by Michael J. Crawley, John Wiley &amp; Sons, Ltd, United Kingdom, 2013. http://www.bio.ic.ac.uk/research/mjcraw/therbook/index.htm Mathematical Statistics with Applications by Dennis D. Wackerly and William Mendenhall III and Richard L. Scheaffer, Thomson Learning, 2002. The bookdown package in R: https://bookdown.org/yihui/bookdown/ "],
["references.html", "References", " References "]
]
